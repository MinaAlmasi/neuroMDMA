{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059acd4d",
   "metadata": {},
   "source": [
    "# EEG Statistics\n",
    "\n",
    "### FaceWord experiment\n",
    "Using some of the FaceWord data you collected, we are going to go through a couple of different ways to discern whether the difference in signal between your conditions is statistically significant. You can go through the notebook using your FaceWord data or the data from your own experiment, that's up to you :-)\n",
    "\n",
    "## Epochs\n",
    "We need epochs for the statistical tests, so you can insert your preprocessed data from earlier tutorials under this section. I'll read in some epochs from your FaceWord which I prepared earlier and I'll be looking at the words/images contrast.\n",
    "\n",
    "(Tip: You can save the epochs object you created in your preprocessing notebook by using epochs.save('your_epochs-epo.fif'))\n",
    "\n",
    "(Extra tip: You can run terminal commands from cells using the os.system() function or simply writing an exclamation mark before the command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780bc5a-03de-4c82-8102-2cdd74b342ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import os\n",
    "import pip\n",
    "!python -m pip install mne\n",
    "os.system('python -m pip install scikit-learn')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0c6d7-d19d-4c18-80e2-621a3ab28074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = mne.read_epochs('/work/57899/faceword-epo.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559befef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_pos_epochs = epochs['Word']\n",
    "word_neg_epochs = epochs['Image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ee979-9344-4e74-ad59-08eb6a89d58b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Windowed mean\n",
    "Now we have our two conditions: trials with words vs images. One of the simplest way in which we can determine whether the signal in our two conditions are statistically significant is by:\n",
    "\n",
    "1) Segmenting our data using only certain channels in a specific time window. Keep in mind that which time window and channels should be established a priori, for instance according to the literature. \n",
    "2) Taking the mean of that window across channels and and samples.\n",
    "3) Running statistical tests on the windowed means from the two conditions.\n",
    "\n",
    "In an experiment with multiple participants we would also average over trials from individual participants, in order to only have one data point per participant (and thereby avoid multiple comparisons). However, since we have one participant, we can keep one dimension of the individual data, i.e. the trials.\n",
    "\n",
    "### T-test\n",
    "We can now do a t-test on the trials from the two conditions, to establish whether the means of the two groups are statistically significant.\n",
    "\n",
    "We can use the get_data() function to get the numerical values of the signal (in microvolts) for the t-test. tmin and tmax are used to define the size of the window, and the picks are the channels that we expect to see an effect in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c73fca-e820-4191-b071-19e144f3a5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_pos_data = word_pos_epochs.get_data(picks=['O1', 'Oz', 'O2'], tmin=.1, tmax=.2)\n",
    "print(word_pos_data.shape)\n",
    "\n",
    "word_neg_data = word_neg_epochs.get_data(picks=['O1', 'Oz', 'O2'], tmin=.1, tmax=.2)\n",
    "print(word_neg_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad0aea-7a65-4dae-ade6-b3ca4cbf4e10",
   "metadata": {},
   "source": [
    "Investigating the resulting data; how many dimensions does the data have? What do you think they represent (i.e. which dimension is channels, trials, etc.)?\n",
    "\n",
    "Now we can average over the data so we only have the trials dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945d0f9-517f-40e2-85bf-3e9487c1c02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_pos_mean = np.mean(word_pos_data, axis=2) # averaging over the third dimension of the data\n",
    "print(word_pos_mean.shape)\n",
    "\n",
    "word_pos_mean = np.mean(word_pos_mean, axis=1) # averaging over the second dimension of the data\n",
    "print(word_pos_mean.shape)\n",
    "\n",
    "word_neg_mean = np.mean(word_neg_data, axis=2)\n",
    "word_neg_mean = np.mean(word_neg_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ab517-4410-414c-a900-7ecd5dd9068a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "import statistics as stats\n",
    "\n",
    "st.ttest_ind(a=word_pos_mean, b=word_neg_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6192-4a8e-458f-80f4-154104fed482",
   "metadata": {},
   "source": [
    "### Mixed effects model\n",
    "By averaging over multiple dimensions of our data, we are of couse throwing away some information. If we had multiple participants, we could add the trials back into the mix by using a mixed-effects model. Since we already have trials, we could add the samples or channels as random effects in a mixed-effects model.\n",
    "\n",
    "We can export the data in a csv format so you have the option of doing a bit of modelling in R if you would like to ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7fed7-5cc3-4a39-ae2a-bd3e1830e462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape = word_pos_data.shape\n",
    "index = pd.MultiIndex.from_product([range(s)for s in shape])\n",
    "word_pos = pd.DataFrame({'word_pos_data': word_pos_data.flatten()}, index=index).reset_index()\n",
    "word_pos.to_csv('word_pos_data.csv', index=False)\n",
    "\n",
    "shape = word_neg_data.shape\n",
    "index = pd.MultiIndex.from_product([range(s)for s in shape])\n",
    "word_neg = pd.DataFrame({'word_neg_data': word_neg_data.flatten()}, index=index).reset_index()\n",
    "word_neg.to_csv('word_neg_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588eb06-3b8c-4516-b8c5-045f2baa2ed0",
   "metadata": {},
   "source": [
    "## Permutation test\n",
    "The null hypothesis (H0) is that the data in the two conditions comes from the same probability distribution (i.e. they are interchangeable). In order to test this we scramble the data in the conditions n amount of times to get an idea of what distributions of cluster sizes we would expect if there is no difference between conditions. Based on this distribution we can establish how large a cluster should be to cross our significance level (e.g. 0.05) and then compare this to the clusters based on our conditions. If the highest value from our clusters is larger, this suggests that the data in the conditions are not interchangeable (i.e. the difference between them is significant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98837e2-92ad-42f6-bc12-6de37b5774fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the data from our conditions\n",
    "X = [epochs[k].get_data() for k in ['Word', 'Image']]\n",
    "print(X[0].shape)\n",
    "\n",
    "# transposing\n",
    "X = [np.transpose(x, (0, 2, 1)) for x in X]\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d807eac-9ab2-40d1-accd-7fb361b86ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# finding channel adjacency: informs us which channels are next to each other (for co-localisation)\n",
    "adjacency, ch_names = mne.channels.find_ch_adjacency(epochs.info, ch_type='eeg')\n",
    "\n",
    "# plotting between-sensor adjacency\n",
    "plt.imshow(adjacency.toarray(), cmap='gray',\n",
    "           interpolation='nearest')\n",
    "plt.xlabel('{} EEG'.format(len(ch_names)))\n",
    "plt.ylabel('{} EEG'.format(len(ch_names)))\n",
    "plt.title('Between-sensor adjacency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b6d52-79a3-4ff4-9107-690030f0c983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set family-wise p-value\n",
    "p_accept = 0.05\n",
    "\n",
    "# running the permutation test with 2000 permutations and a random seed of 4\n",
    "cluster_stats = mne.stats.spatio_temporal_cluster_test(X, n_permutations=1000, tail=0,\n",
    "                                             n_jobs=-1, buffer_size=None, adjacency=adjacency, seed=4)\n",
    "\n",
    "# selecting clusters with significant p-values\n",
    "T_obs, clusters, p_values, _ = cluster_stats\n",
    "good_cluster_inds = np.where(p_values < p_accept)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba1545-1745-4a3a-805c-c2fc2c76e522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(p_values)\n",
    "print(good_cluster_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a4b1f-41d4-4c66-8677-2a6f665c6c14",
   "metadata": {},
   "source": [
    "The code for this plot is a bit long and complex but you don't have to go through it all, just swap in your conditions in the first couple of lines :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d5253-3129-42ba-8f67-0e6dca70c755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# configuration of variables for visualisation\n",
    "colors = {\"Word\": \"crimson\", \"Image\": 'steelblue'}\n",
    "\n",
    "# organising data for plotting\n",
    "evokeds = {cond: epochs[cond].average() for cond in ['Word', 'Image']} \n",
    "\n",
    "# looping over clusters\n",
    "for i_clu, clu_idx in enumerate(good_cluster_inds):\n",
    "    # unpacking cluster information\n",
    "    time_inds, space_inds = np.squeeze(clusters[clu_idx])\n",
    "    ch_inds = np.unique(space_inds)\n",
    "    time_inds = np.unique(time_inds)\n",
    "\n",
    "    # topography for F stat\n",
    "    f_map = T_obs[time_inds, ...].mean(axis=0)\n",
    "\n",
    "    # getting signals at the sensors contributing to the cluster\n",
    "    sig_times = epochs.times[time_inds]\n",
    "\n",
    "    # creating spatial mask\n",
    "    mask = np.zeros((f_map.shape[0], 1), dtype=bool)\n",
    "    mask[ch_inds, :] = True\n",
    "\n",
    "    # initialising the figure\n",
    "    fig, ax_topo = plt.subplots(1, 1, figsize=(10, 3))\n",
    "\n",
    "    # plotting average test statistic and mark significant sensors\n",
    "    f_evoked = mne.EvokedArray(f_map[:, np.newaxis], epochs.info, tmin=0)\n",
    "    f_evoked.plot_topomap(times=0, mask=mask, axes=ax_topo, cmap='Reds',\n",
    "                          vmin=np.min, vmax=np.max, show=False,\n",
    "                          colorbar=False, mask_params=dict(markersize=10))\n",
    "    image = ax_topo.images[0]\n",
    "\n",
    "    # creating additional axes (for ERF and colorbar)\n",
    "    divider = make_axes_locatable(ax_topo)\n",
    "\n",
    "    # adding axes for colourbar\n",
    "    ax_colorbar = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    plt.colorbar(image, cax=ax_colorbar)\n",
    "    ax_topo.set_xlabel(\n",
    "        'Averaged F-map ({:0.3f} - {:0.3f} s)'.format(*sig_times[[0, -1]]))\n",
    "\n",
    "    # adding new axis for time courses and plot time courses\n",
    "    ax_signals = divider.append_axes('right', size='300%', pad=1.2)\n",
    "    title = 'Cluster #{0}, {1} sensor'.format(i_clu + 1, len(ch_inds))\n",
    "    if len(ch_inds) > 1:\n",
    "        title += \"s (mean)\"\n",
    "    mne.viz.plot_compare_evokeds(evokeds, title=title, picks=ch_inds, axes=ax_signals,\n",
    "                         colors=colors, show=False,\n",
    "                         split_legend=True, truncate_yaxis='auto')\n",
    "\n",
    "    # plotting temporal cluster extent\n",
    "    ymin, ymax = ax_signals.get_ylim()\n",
    "    ax_signals.fill_betweenx((ymin, ymax), sig_times[0], sig_times[-1],\n",
    "                             color='orange', alpha=0.3)\n",
    "\n",
    "    # clean-up\n",
    "    mne.viz.tight_layout(fig=fig)\n",
    "    fig.subplots_adjust(bottom=.05)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad5c30fea5466ca3b04feb7e0a99857a92ee68be4c3f4107148abce45ea17a05"
  },
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
